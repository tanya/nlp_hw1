{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1\n",
    "\n",
    "\n",
    "Tanya Balaraju\n",
    "\n",
    "Professor Matthew Stone\n",
    "\n",
    "CS 533  \n",
    "\n",
    "Due March 19\n",
    "\n",
    "\n",
    "*Modification of classifier-patterns.ipynb*\n",
    "\n",
    "In this notebook, the TFIDF representation of the 20 Newsgroups dataset will be used to hypothesize about the classification of two newsgroups in the dataset. The hypothesis will then be evaluated using the classification methods from the classifier-patterns notebook.\n",
    "\n",
    "**Important Notes: **\n",
    "\n",
    "- All code in this notebook is in Python 3. Modifications will need to be made in order to run the notebook using Python 2. \n",
    "- The Experiment class has been modified; xval() now returns the average accuracy.\n",
    "- All values mentioned in the \"text\" sections of the notebook are returned from the first iteration of the experiments. Running this notebook again may lead to slight variations in the accuracies, because the experiments are being rerun. \n",
    "- The format of the filesystem used in this experiment is as follows:\n",
    "    - news/20news-bydate-train/pos_NUMBER is the directory containing the \"positive classification\" training set for newsgroup NUMBER.\n",
    "    - news/20news-bydate-train/neg_NUMBER is the directory containing the \"positive classification\" training set for newsgroup NUMBER (i.e., files from all newsgroups other than newsgroup NUMBER).\n",
    "    - The above formatting applies to the testing set as well. \n",
    "    - To run this notebook, the following subdirectories will be necessary in both 20news-bydate-train/ and 20news-bydate-test/: pos_15, neg_15, pos_17, neg_17, pos_0, neg_0, and pos_6, and neg_6. *The correctly-formatted directory has been included in the file news.tgz.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. General Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below remains consistent throughout all of the experiments (modified from classifier-patterns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import nltk\n",
    "#nltk.download()\n",
    "import re\n",
    "import itertools\n",
    "import vocabulary\n",
    "import newsreader\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from numpy import add, array, dot\n",
    "from numpy.linalg import norm\n",
    "from scipy.sparse import csr_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_file, vocab_file_type = \"reviews-vocab.pkl\", \"pickle\"\n",
    "\n",
    "embedding_file, embedding_dimensions, embedding_cache = \\\n",
    "    \"glove.6B.50d.txt\", 50, \"news-embedding.npz\"\n",
    "\n",
    "all_data, train_dir, dev_dir, test_dir = \\\n",
    "    \"news/\", \"news/20news-bydate-train/\", None, \"news/20news-bydate-test/\"\n",
    "\n",
    "has_bad_metadata = True\n",
    "made_vocabulary = True\n",
    "\n",
    "if made_vocabulary :\n",
    "    v = vocabulary.Vocabulary.load(vocab_file, file_type=vocab_file_type)\n",
    "else: \n",
    "    tokens = newsreader.all_textfile_tokens(all_data, strip_metadata=has_bad_metadata)                                            \n",
    "    v = vocabulary.Vocabulary.from_iterable(tokens, file_type=vocab_file_type)\n",
    "    v.save(vocab_file)\n",
    "v.stop_growth()\n",
    "\n",
    "made_embedding = True\n",
    "if made_embedding :\n",
    "    e = newsreader.load_sparse_csr(embedding_cache)\n",
    "else: \n",
    "    e = newsreader.build_sparse_embedding(v, embedding_file, embedding_dimensions)\n",
    "    newsreader.save_sparse_csr(embedding_cache, e)\n",
    "\n",
    "targets = []\n",
    "def selected(name) :\n",
    "    if not targets:\n",
    "        return True\n",
    "    if any(t.startswith(name) for t in targets) :\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def use_default_features(vocab) :\n",
    "    return lambda data: vocab\n",
    "\n",
    "def count_features(features, gen_tokens) :\n",
    "    for t in gen_tokens :\n",
    "        r = features.add(t)\n",
    "        if r :\n",
    "            yield r \n",
    "\n",
    "def make_boolean_features(feature_counter) :\n",
    "    def collect_features(features, gen_tokens) :\n",
    "        seen = set()\n",
    "        for f in feature_counter(features, gen_tokens) :\n",
    "            seen.add(f)\n",
    "        for f in seen :\n",
    "            yield f\n",
    "    return collect_features\n",
    "\n",
    "class Experiment(object) :\n",
    "    '''Organize the process of getting data, building a classifier,\n",
    "    and exploring new representations'''\n",
    "    \n",
    "    def __init__(self, data, comment, classifier, cdesc) :\n",
    "        'set up the problem of learning a classifier from a data manager'\n",
    "        self.data = data\n",
    "        self.comment = comment\n",
    "        self.classifier = classifier\n",
    "        self.cdesc = cdesc\n",
    "        self.initialized = False\n",
    "        \n",
    "    def initialize(self) :\n",
    "        'materialize the training data, dev data and test data as matrices'\n",
    "        if not self.initialized :\n",
    "            self.train_X, self.train_y = self.data.training_data()\n",
    "            self.dev_X, self.dev_y = self.data.dev_data()\n",
    "            self.test_X, self.test_y = self.data.test_data()\n",
    "            self.initialized = True\n",
    "        \n",
    "    def fit_and_validate(self, report=True) :\n",
    "        'train the classifier and assess predictions on dev data'\n",
    "        if not self.initialized :\n",
    "            self.initialize()\n",
    "        self.classifier.fit(self.train_X, self.train_y)\n",
    "        self.dev_predictions = self.classifier.predict(self.dev_X)\n",
    "        self.accuracy = sklearn.metrics.accuracy_score(self.dev_y, self.dev_predictions)\n",
    "        if report :\n",
    "            print(\"{}\\nclassified by {}\\naccuracy {}\".format(self.comment, self.cdesc, self.accuracy))\n",
    "            \n",
    "    def xval(self, folds=20, report=True) :\n",
    "        #print(\"Running xval\")\n",
    "        accuracies = []\n",
    "        for i in range(folds) :\n",
    "            self.fit_and_validate(report=False)\n",
    "            accuracies.append(self.accuracy)\n",
    "        if report :\n",
    "            msg = \"{}\\nclassified by {}\\naverage accuracy {} (std {})\"\n",
    "            print(msg.format(self.comment, self.cdesc, \n",
    "                             sum(accuracies)/folds,\n",
    "                             np.std(accuracies)))\n",
    "            #print(\"{},{}\".format(accuracies,folds))\n",
    "            return sum(accuracies)/folds\n",
    "    \n",
    "    @classmethod\n",
    "    def transform(cls, expt, operation, description, classifier, cdesc) :\n",
    "        'use operation to transform the data from expt and set up new classifier'\n",
    "        if not expt.initialized :\n",
    "            expt.initialize()\n",
    "        result = cls(expt.data, expt.comment + '\\n' + description, classifier, cdesc)\n",
    "        result.train_X, result.train_y = operation(expt.train_X, expt.train_y, 'train')\n",
    "        result.dev_X, result.dev_y = operation(expt.dev_X, expt.dev_y, 'dev')\n",
    "        result.test_X, result.test_y = operation(expt.test_X, expt.test_y, 'test')\n",
    "        result.initialized = True\n",
    "        return result\n",
    "\n",
    "def wang_manning_weights(expt) :\n",
    "    Xyes = expt.train_X[expt.train_y ==1, :]\n",
    "    Xno = expt.train_X[expt.train_y != 1, :] \n",
    "    yesrates = np.log((Xyes.getnnz(axis=0) + 1.) / Xyes.shape[1])\n",
    "    norates = np.log((Xno.getnnz(axis=0) + 1.) / Xno.shape[1])\n",
    "    W = scipy.sparse.diags(yesrates - norates, 0)\n",
    "    return lambda X, y, c: (X.dot(W), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More function definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idf_weights(expt) :\n",
    "    idf = np.log((expt.train_X.shape[1] + 1.) / (expt.train_X.getnnz(axis=0) + 1.))\n",
    "    W = scipy.sparse.diags(idf, 0)\n",
    "    return lambda X, y, c: (X.dot(W), y)\n",
    "\n",
    "def add_embeddings(expt, embeddings, scale=True, stack=True) :\n",
    "    extra_features = expt.train_X.shape[1] - embeddings.shape[0]\n",
    "    if extra_features > 0 :\n",
    "        Z = scipy.sparse.csr_matrix((extra_features, embeddings.shape[1]))\n",
    "        W = scipy.sparse.vstack([embeddings, Z])\n",
    "    else: \n",
    "        W = embeddings\n",
    "    if scale :\n",
    "        scaler = StandardScaler(with_mean=False)\n",
    "        scaler.fit(expt.train_X.dot(W))\n",
    "    def operation(X, y, s) :\n",
    "        if scale:\n",
    "            new_features = scaler.transform(X.dot(W))\n",
    "        else :\n",
    "            new_features = X.dot(W)\n",
    "        if stack :\n",
    "            all_features = scipy.sparse.hstack([X, new_features]).tocsr()\n",
    "        else :\n",
    "            all_features = new_features\n",
    "        return (all_features, y)\n",
    "    return operation\n",
    "\n",
    "def dimensionality_reduction(expt, dimensions) :\n",
    "    _, _, wrt = scipy.sparse.linalg.svds(expt.train_X, k=dimensions, \n",
    "                                         return_singular_vectors='vh')\n",
    "    return add_embeddings(expt, np.transpose(wrt), stack=False)\n",
    "\n",
    "def use_bigram_features(data) :\n",
    "    f = vocabulary.Vocabulary.load(vocab_file, file_type=vocab_file_type)\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    word_fd = nltk.FreqDist(data.all_train_tokens())\n",
    "    bigram_fd = nltk.FreqDist(nltk.bigrams(data.all_train_tokens()))\n",
    "    finder = nltk.collocations.BigramCollocationFinder(word_fd, bigram_fd)\n",
    "    finder.apply_freq_filter(5)\n",
    "    collocations = filter(lambda g: g[1] > 0, finder.score_ngrams(bigram_measures.pmi))\n",
    "    for (w1, w2), _ in collocations:\n",
    "        f.add(w1 + \" \" + w2)\n",
    "    f.stop_growth()\n",
    "    return f\n",
    "\n",
    "def count_bigram_features(features, gen_tokens) :\n",
    "    prev = None\n",
    "    for t in gen_tokens :\n",
    "        r = features.add(t)\n",
    "        if r :\n",
    "            yield r\n",
    "            if prev :\n",
    "                r = features.add(prev + \" \" + t)\n",
    "                if r : \n",
    "                    yield r\n",
    "            prev = t\n",
    "            \n",
    "def limit_training(percent) :\n",
    "    def operation(X, y, s) :\n",
    "        if s != 'train' :\n",
    "            return (X, y)\n",
    "        data_to_take = int(X.shape[0] * percent)\n",
    "        indices = np.random.choice(X.shape[0], \n",
    "                                      size=data_to_take,\n",
    "                                      replace=False)\n",
    "        return (X[indices,:], y[indices])\n",
    "    return operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pair Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Similar Newsgroups \n",
    "\n",
    "The first assumption made is that the newsgroups \"Religion: Miscellaneous\" (class 19) and \"Religion: Christian\" (class 15) are probably fairly similar. Running the experiments on these groups requires some setup: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#files in class 15\n",
    "class15_pos = \"pos_15\"\n",
    "#files in class 19\n",
    "class15_neg = \"neg_15\"\n",
    "\n",
    "count15_data = newsreader.DataManager(train_dir + class15_pos,\n",
    "                                       train_dir + class15_neg,\n",
    "                                       test_dir + class15_pos,\n",
    "                                       test_dir + class15_neg,\n",
    "                                       use_default_features(v),\n",
    "                                       count_features,\n",
    "                                       dev_dir + class15_pos if dev_dir else None,\n",
    "                                       dev_dir + class15_neg if dev_dir else None,\n",
    "                                       strip_metadata=has_bad_metadata)\n",
    "\n",
    "\n",
    "boolean_data_15 = newsreader.DataManager(train_dir + class15_pos,\n",
    "                                         train_dir + class15_neg,\n",
    "                                         test_dir + class15_pos,\n",
    "                                         test_dir + class15_neg,\n",
    "                                         use_default_features(v),\n",
    "                                         make_boolean_features(count_features),\n",
    "                                         dev_dir + class15_pos if dev_dir else None,\n",
    "                                         dev_dir + class15_neg if dev_dir else None,\n",
    "                                         strip_metadata=has_bad_metadata)\n",
    "\n",
    "count15_data.initialize(build_cache=True)\n",
    "boolean_data_15.initialize(build_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we run the first experiment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running xval\n",
      "news/: pos_15 vs neg_15, using word count features\n",
      "classified by logistic regression\n",
      "average accuracy 0.6944444444444445 (std 0.011790522757503785)\n",
      "[0.6767676767676768, 0.71212121212121215, 0.71717171717171713, 0.6767676767676768, 0.70707070707070707, 0.6767676767676768, 0.69191919191919193, 0.69696969696969702, 0.70202020202020199, 0.68686868686868685, 0.68686868686868685, 0.70202020202020199, 0.68181818181818177, 0.70202020202020199, 0.70707070707070707, 0.69191919191919193, 0.69696969696969702, 0.70202020202020199, 0.68181818181818177, 0.69191919191919193],20\n"
     ]
    }
   ],
   "source": [
    "if selected(\"expt_10_class15\"):\n",
    "    expt_10_c15 = Experiment(count15_data,\n",
    "                       \"{}: {} vs {}, using word count features\".format(all_data, class15_pos, class15_neg),\n",
    "                       sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                       \"logistic regression\")\n",
    "    expt_10_c15.initialize()\n",
    "    expt_10_c15.xval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At about 70%, the accuracy of the classification using word count features appears to be fairly low, although this was a high-performing method in the original classifier-patterns notebook. This is unsurprising; the two classes are plausibly similar enough to lead to such an outcome. \n",
    "\n",
    "Now, we test further: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running xval\n",
      "news/: pos_15 vs neg_15, using word presence/absence features\n",
      "classified by logistic regression\n",
      "average accuracy 0.7161616161616162 (std 0.016334304675551747)\n",
      "[0.75252525252525249, 0.72222222222222221, 0.71717171717171713, 0.70202020202020199, 0.68181818181818177, 0.71717171717171713, 0.69696969696969702, 0.72727272727272729, 0.72222222222222221, 0.71212121212121215, 0.74242424242424243, 0.72727272727272729, 0.70707070707070707, 0.69696969696969702, 0.72727272727272729, 0.70202020202020199, 0.70707070707070707, 0.72222222222222221, 0.70707070707070707, 0.73232323232323238],20\n"
     ]
    }
   ],
   "source": [
    "if selected(\"expt_11_class15\") :\n",
    "    expt_11_c15 = Experiment(boolean_data_15,\n",
    "                         \"{}: {} vs {}, using word presence/absence features\".format(all_data, class15_pos, class15_neg),\n",
    "                         sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                         \"logistic regression\")\n",
    "    expt_11_c15.initialize()\n",
    "    expt_11_c15.xval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although slightly better than the word-count classification, the average accuracy in this case is still fairly low, at 71%. \n",
    "\n",
    "We continue with the remaining experiments from classifier-patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running xval\n",
      "news/: pos_15 vs neg_15, using word presence/absence features\n",
      "features weighted by evidence they give of class\n",
      "classified by logistic regression\n",
      "average accuracy 0.7414141414141414 (std 0.017466279258374323)\n",
      "[0.72727272727272729, 0.75252525252525249, 0.74242424242424243, 0.70202020202020199, 0.74747474747474751, 0.74242424242424243, 0.73737373737373735, 0.76262626262626265, 0.75252525252525249, 0.73232323232323238, 0.73232323232323238, 0.73737373737373735, 0.70707070707070707, 0.74242424242424243, 0.75252525252525249, 0.77777777777777779, 0.75252525252525249, 0.76262626262626265, 0.73737373737373735, 0.72727272727272729],20\n"
     ]
    }
   ],
   "source": [
    "if selected(\"expt_11_20_c15\") :\n",
    "    expt_11_20_c15 = Experiment.transform(expt_11_c15,\n",
    "                             wang_manning_weights(expt_11_c15),\n",
    "                             \"features weighted by evidence they give of class\",\n",
    "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                             \"logistic regression\")\n",
    "    expt_11_20_c15.xval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see an improvement in accuracy after introducing Wang-Manning weights, bringing the accuracy to 75%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running xval\n",
      "news/: pos_15 vs neg_15, using word count features\n",
      "features weighted by inverse document frequency\n",
      "classified by logistic regression\n",
      "average accuracy 0.6941919191919192 (std 0.014238035716172182)\n",
      "[0.70202020202020199, 0.68686868686868685, 0.69696969696969702, 0.72727272727272729, 0.70202020202020199, 0.69191919191919193, 0.6767676767676768, 0.68181818181818177, 0.69191919191919193, 0.69191919191919193, 0.69696969696969702, 0.67171717171717171, 0.71212121212121215, 0.68686868686868685, 0.66666666666666663, 0.70202020202020199, 0.71212121212121215, 0.68181818181818177, 0.70202020202020199, 0.70202020202020199],20\n"
     ]
    }
   ],
   "source": [
    "if selected(\"expt_10_20_c15\") :\n",
    "    expt_10_20_c15 = Experiment.transform(expt_10_c15,\n",
    "                             idf_weights(expt_10_c15),\n",
    "                             \"features weighted by inverse document frequency\",\n",
    "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                             \"logistic regression\")\n",
    "    expt_10_20_c15.xval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Inverse Document Frequency (IDF) for feature weighting does not seem to improve the original accuracy of Experiment 10; it remains around 70%.\n",
    "\n",
    "Now, we use word embeddings and LSA transformations on experiments 10 and 11: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running xval\n",
      "news/: pos_15 vs neg_15, using word count features\n",
      "enriched via word embeddings\n",
      "classified by logistic regression\n",
      "average accuracy 0.7012626262626263 (std 0.014220109318485944)\n",
      "[0.70707070707070707, 0.72727272727272729, 0.71717171717171713, 0.70202020202020199, 0.68686868686868685, 0.70202020202020199, 0.70707070707070707, 0.71212121212121215, 0.69696969696969702, 0.69696969696969702, 0.71212121212121215, 0.71717171717171713, 0.70202020202020199, 0.71717171717171713, 0.69191919191919193, 0.68686868686868685, 0.70202020202020199, 0.67171717171717171, 0.67171717171717171, 0.69696969696969702],20\n",
      "Running xval\n",
      "news/: pos_15 vs neg_15, using word count features\n",
      "transformed via LSA(100)\n",
      "classified by logistic regression\n",
      "average accuracy 0.6527777777777777 (std 0.026430842393878332)\n",
      "[0.66161616161616166, 0.6767676767676768, 0.63131313131313127, 0.63636363636363635, 0.57070707070707072, 0.66161616161616166, 0.64646464646464652, 0.65151515151515149, 0.63131313131313127, 0.65656565656565657, 0.64646464646464652, 0.6767676767676768, 0.66666666666666663, 0.66666666666666663, 0.6767676767676768, 0.61616161616161613, 0.69191919191919193, 0.66666666666666663, 0.6767676767676768, 0.64646464646464652],20\n",
      "Running xval\n",
      "news/: pos_15 vs neg_15, using word presence/absence features\n",
      "enriched via word embeddings\n",
      "classified by logistic regression\n",
      "average accuracy 0.7088383838383839 (std 0.010384285451996008)\n",
      "[0.70202020202020199, 0.69191919191919193, 0.69191919191919193, 0.70202020202020199, 0.73232323232323238, 0.72222222222222221, 0.71212121212121215, 0.71212121212121215, 0.72222222222222221, 0.70707070707070707, 0.70707070707070707, 0.71212121212121215, 0.69696969696969702, 0.71212121212121215, 0.70707070707070707, 0.71212121212121215, 0.69191919191919193, 0.71717171717171713, 0.71212121212121215, 0.71212121212121215],20\n",
      "Running xval\n",
      "news/: pos_15 vs neg_15, using word presence/absence features\n",
      "features weighted by evidence they give of class\n",
      "enriched via word embeddings\n",
      "classified by logistic regression\n",
      "average accuracy 0.7373737373737372 (std 0.009174698042719679)\n",
      "[0.73737373737373735, 0.74242424242424243, 0.71717171717171713, 0.73232323232323238, 0.72222222222222221, 0.73737373737373735, 0.74747474747474751, 0.72222222222222221, 0.73232323232323238, 0.74747474747474751, 0.73737373737373735, 0.74242424242424243, 0.74242424242424243, 0.73232323232323238, 0.73232323232323238, 0.75252525252525249, 0.74747474747474751, 0.74747474747474751, 0.73737373737373735, 0.73737373737373735],20\n"
     ]
    }
   ],
   "source": [
    "if selected(\"expt_10_211_c15\") :\n",
    "    expt_10_211_c15 = Experiment.transform(expt_10_c15,\n",
    "                             add_embeddings(expt_10_c15, e),\n",
    "                             \"enriched via word embeddings\",\n",
    "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                            \"logistic regression\")\n",
    "    expt_10_211_c15.xval()\n",
    "\n",
    "if selected (\"expt_10_212_c15\") :\n",
    "    expt_10_212_c15 = Experiment.transform(expt_10_c15,\n",
    "                             dimensionality_reduction(expt_10_c15, 100),\n",
    "                             \"transformed via LSA(100)\",\n",
    "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                           \"logistic regression\")\n",
    "    expt_10_212_c15.xval()\n",
    "    \n",
    "if selected(\"expt_11_211_\") :\n",
    "    expt_11_211_c15 = Experiment.transform(expt_11_c15,\n",
    "                             add_embeddings(expt_11_c15, e),\n",
    "                             \"enriched via word embeddings\",\n",
    "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                            \"logistic regression\")\n",
    "    expt_11_211_c15.xval()\n",
    "\n",
    "if selected(\"expt_11_20_211_c15\") :\n",
    "    expt_11_20_211_c15 = Experiment.transform(expt_11_20_c15,\n",
    "                             add_embeddings(expt_11_20_c15, e),\n",
    "                             \"enriched via word embeddings\",\n",
    "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                            \"logistic regression\")\n",
    "    expt_11_20_211_c15.xval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 10 is not significantly improved by the use of word embeddings, and it is worsened by the LSA transformation (accuracy falls to 65%). Experiment 11 is also not significantly improved with word embeddings, but weighting the features by the evidence they give of their class increases the base accuracy to 74%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running xval\n",
      "news/: pos_15 vs neg_15, using word and bigram presence/absence features\n",
      "classified by logistic regression\n",
      "average accuracy 0.7045454545454546 (std 0.018314610469160362)\n",
      "[0.68181818181818177, 0.71212121212121215, 0.71212121212121215, 0.67171717171717171, 0.71212121212121215, 0.73232323232323238, 0.72222222222222221, 0.69191919191919193, 0.73737373737373735, 0.71717171717171713, 0.68181818181818177, 0.69696969696969702, 0.70707070707070707, 0.71717171717171713, 0.68181818181818177, 0.69696969696969702, 0.69696969696969702, 0.68181818181818177, 0.71212121212121215, 0.72727272727272729],20\n",
      "\n",
      "\n",
      "Running xval\n",
      "news/: pos_15 vs neg_15, using word and bigram presence/absence features\n",
      "features weighted by evidence they give of class\n",
      "classified by logistic regression\n",
      "average accuracy 0.7510101010101009 (std 0.020829889522526533)\n",
      "[0.78787878787878785, 0.75252525252525249, 0.72222222222222221, 0.75252525252525249, 0.77272727272727271, 0.74242424242424243, 0.76767676767676762, 0.73232323232323238, 0.73737373737373735, 0.73737373737373735, 0.74747474747474751, 0.75252525252525249, 0.73737373737373735, 0.75757575757575757, 0.76767676767676762, 0.75252525252525249, 0.71717171717171713, 0.75252525252525249, 0.72727272727272729, 0.80303030303030298],20\n"
     ]
    }
   ],
   "source": [
    "bigram_data = newsreader.DataManager(train_dir + class15_pos,\n",
    "                                       train_dir + class15_neg,\n",
    "                                       test_dir + class15_pos,\n",
    "                                       test_dir + class15_neg,\n",
    "                                       use_bigram_features,\n",
    "                                       make_boolean_features(count_bigram_features),\n",
    "                                       dev_dir + class15_pos if dev_dir else None,\n",
    "                                       dev_dir + class15_neg if dev_dir else None,\n",
    "                                       strip_metadata=has_bad_metadata)\n",
    "\n",
    "bigram_data.initialize(build_cache=True)\n",
    "\n",
    "if selected(\"expt_22_c15\") :\n",
    "    expt_22_c15 = Experiment(bigram_data,\n",
    "                   \"{}: {} vs {}, using word and bigram presence/absence features\".format(all_data, class15_pos, class15_neg),\n",
    "                   sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                   \"logistic regression\")\n",
    "    expt_22_c15.initialize()\n",
    "    expt_22_c15.xval()\n",
    "\n",
    "print ('\\n')\n",
    "if selected(\"expt_22_220_c15\") :\n",
    "    expt_22_220_c15 = Experiment.transform(expt_22_c15,\n",
    "                             wang_manning_weights(expt_22_c15),\n",
    "                             \"features weighted by evidence they give of class\",\n",
    "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                             \"logistic regression\")\n",
    "    expt_22_220_c15.initialize()\n",
    "    expt_22_220_c15.xval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the use of word and bigram presence/absence features does not improve the experiment, adding the weighting of features by the evidence they give of their class increases the accuracy to around 75%.\n",
    "\n",
    "Finally, we limit the training data and observe the changes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running xval\n",
      "news/: pos_15 vs neg_15, using word presence/absence features\n",
      "considering 10% training data\n",
      "classified by logistic regression\n",
      "average accuracy 0.5979797979797981 (std 0.029118253142221363)\n",
      "[0.57070707070707072, 0.58080808080808077, 0.57070707070707072, 0.53535353535353536, 0.64141414141414144, 0.58080808080808077, 0.59595959595959591, 0.61111111111111116, 0.64141414141414144, 0.63131313131313127, 0.59090909090909094, 0.60101010101010099, 0.64646464646464652, 0.61616161616161613, 0.58585858585858586, 0.62121212121212122, 0.61111111111111116, 0.55555555555555558, 0.59595959595959591, 0.5757575757575758],20\n",
      "\n",
      "\n",
      "Running xval\n",
      "news/: pos_15 vs neg_15, using word presence/absence features\n",
      "enriched via word embeddings\n",
      "considering 10% training data\n",
      "classified by logistic regression\n",
      "average accuracy 0.620959595959596 (std 0.03115285635598802)\n",
      "[0.58080808080808077, 0.57070707070707072, 0.6262626262626263, 0.64646464646464652, 0.64141414141414144, 0.65656565656565657, 0.65151515151515149, 0.65656565656565657, 0.64646464646464652, 0.59595959595959591, 0.57070707070707072, 0.62121212121212122, 0.58585858585858586, 0.59090909090909094, 0.63636363636363635, 0.58080808080808077, 0.62121212121212122, 0.63636363636363635, 0.63131313131313127, 0.67171717171717171],20\n",
      "\n",
      "\n",
      "Running xval\n",
      "news/: pos_15 vs neg_15, using word presence/absence features\n",
      "features weighted by evidence they give of class\n",
      "considering 10% training data\n",
      "classified by logistic regression\n",
      "average accuracy 0.6424242424242423 (std 0.03861572697700055)\n",
      "[0.61111111111111116, 0.61111111111111116, 0.64646464646464652, 0.61111111111111116, 0.58080808080808077, 0.60101010101010099, 0.69191919191919193, 0.58080808080808077, 0.68686868686868685, 0.70707070707070707, 0.65151515151515149, 0.63636363636363635, 0.68686868686868685, 0.64646464646464652, 0.70707070707070707, 0.66161616161616166, 0.61111111111111116, 0.66666666666666663, 0.63636363636363635, 0.61616161616161613],20\n",
      "\n",
      "\n",
      "Running xval\n",
      "news/: pos_15 vs neg_15, using word presence/absence features\n",
      "features weighted by evidence they give of class\n",
      "enriched via word embeddings\n",
      "considering 10% training data\n",
      "classified by logistic regression\n",
      "average accuracy 0.657070707070707 (std 0.020509030812370658)\n",
      "[0.66666666666666663, 0.67171717171717171, 0.68686868686868685, 0.65151515151515149, 0.65656565656565657, 0.64141414141414144, 0.66161616161616166, 0.65151515151515149, 0.65656565656565657, 0.65151515151515149, 0.6767676767676768, 0.65656565656565657, 0.68181818181818177, 0.65656565656565657, 0.68686868686868685, 0.60101010101010099, 0.64646464646464652, 0.66161616161616166, 0.61616161616161613, 0.66161616161616166],20\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if selected(\"expt_11_231_c15\") :\n",
    "    expt_11_231_c15 = Experiment.transform(expt_11_c15,\n",
    "                             limit_training(0.1),\n",
    "                             \"considering 10% training data\",\n",
    "                            sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                            \"logistic regression\")\n",
    "    expt_11_231_c15.xval()\n",
    "\n",
    "print ('\\n')\n",
    "\n",
    "if selected(\"expt_11_211_232_c15\") :\n",
    "    expt_11_231_232_c15 = Experiment.transform(expt_11_211_c15,\n",
    "                             limit_training(0.1),\n",
    "                             \"considering 10% training data\",\n",
    "                            sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                            \"logistic regression\")\n",
    "    expt_11_231_232_c15.xval()\n",
    "\n",
    "print ('\\n')\n",
    "\n",
    "if selected(\"expt_11_20_233_c15\") :\n",
    "    expt_11_20_232_c15 = Experiment.transform(expt_11_20_c15,\n",
    "                             limit_training(0.1),\n",
    "                             \"considering 10% training data\",\n",
    "                            sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                            \"logistic regression\")\n",
    "    expt_11_20_232_c15.xval()\n",
    "    \n",
    "print ('\\n')\n",
    "\n",
    "if selected(\"expt_11_20_211_234_c15\") :\n",
    "    expt_11_20_211_232_c15 = Experiment.transform(expt_11_20_211_c15,\n",
    "                             limit_training(0.1),\n",
    "                             \"considering 10% training data\",\n",
    "                            sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                            \"logistic regression\")\n",
    "    expt_11_20_211_232_c15.xval()\n",
    "\n",
    "print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the accuracy here is considerably lower. However, the final experiment is only 2 percentage points away from the original accuracy of experiment 10, indicating that the feature weighting and word embeddings did help with the reduced accuracy from the smaller training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Different Newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we test two newsgroups that appear to be different: Newsgroup 17 (Politics: MidEast) and Newsgroup 10 (Sports: Hockey), using the same process as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#files in class 17\n",
    "class17_pos = \"pos_17\"\n",
    "#files in class 10\n",
    "class17_neg = \"neg_17\"\n",
    "\n",
    "count17_data = newsreader.DataManager(train_dir + class17_pos,\n",
    "                                       train_dir + class17_neg,\n",
    "                                       test_dir + class17_pos,\n",
    "                                       test_dir + class17_neg,\n",
    "                                       use_default_features(v),\n",
    "                                       count_features,\n",
    "                                       dev_dir + class17_pos if dev_dir else None,\n",
    "                                       dev_dir + class17_neg if dev_dir else None,\n",
    "                                       strip_metadata=has_bad_metadata)\n",
    "\n",
    "\n",
    "boolean_data_17 = newsreader.DataManager(train_dir + class17_pos,\n",
    "                                         train_dir + class17_neg,\n",
    "                                         test_dir + class17_pos,\n",
    "                                         test_dir + class17_neg,\n",
    "                                         use_default_features(v),\n",
    "                                         make_boolean_features(count_features),\n",
    "                                         dev_dir + class17_pos if dev_dir else None,\n",
    "                                         dev_dir + class17_neg if dev_dir else None,\n",
    "                                         strip_metadata=has_bad_metadata)\n",
    "\n",
    "count17_data.initialize(build_cache=True)\n",
    "boolean_data_17.initialize(build_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments 10 and 11 are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running xval\n",
      "news/: pos_17 vs neg_17, using word count features\n",
      "classified by logistic regression\n",
      "average accuracy 0.9226063829787234 (std 0.01029705355457764)\n",
      "[0.92553191489361697, 0.93617021276595747, 0.92553191489361697, 0.92553191489361697, 0.91489361702127658, 0.92021276595744683, 0.92553191489361697, 0.92553191489361697, 0.93085106382978722, 0.91489361702127658, 0.93085106382978722, 0.92021276595744683, 0.93617021276595747, 0.92553191489361697, 0.90957446808510634, 0.90957446808510634, 0.90957446808510634, 0.89893617021276595, 0.94148936170212771, 0.92553191489361697],20\n",
      "\n",
      "\n",
      "Running xval\n",
      "news/: pos_17 vs neg_17, using word presence/absence features\n",
      "classified by logistic regression\n",
      "average accuracy 0.9284574468085107 (std 0.013610682533961695)\n",
      "[0.90957446808510634, 0.93617021276595747, 0.94148936170212771, 0.9042553191489362, 0.93617021276595747, 0.93085106382978722, 0.93617021276595747, 0.93085106382978722, 0.94680851063829785, 0.92553191489361697, 0.9521276595744681, 0.92021276595744683, 0.94680851063829785, 0.90957446808510634, 0.90957446808510634, 0.94148936170212771, 0.92021276595744683, 0.92021276595744683, 0.93085106382978722, 0.92021276595744683],20\n"
     ]
    }
   ],
   "source": [
    "if selected(\"expt_10_class17\"):\n",
    "    expt_10_c17 = Experiment(count17_data,\n",
    "                       \"{}: {} vs {}, using word count features\".format(all_data, class17_pos, class17_neg),\n",
    "                       sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                       \"logistic regression\")\n",
    "    expt_10_c17.initialize()\n",
    "    expt_10_c17.xval()\n",
    "\n",
    "print ('\\n')\n",
    "\n",
    "if selected(\"expt_11_class17\") :\n",
    "    expt_11_c17 = Experiment(boolean_data_17,\n",
    "                         \"{}: {} vs {}, using word presence/absence features\".format(all_data, class17_pos, class17_neg),\n",
    "                         sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                         \"logistic regression\")\n",
    "    expt_11_c17.initialize()\n",
    "    expt_11_c17.xval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is immediately noticeable that the classification is much more successful with Newsgroups 17 and 10 (92-93% success rate for both word count and word presence/absence features) than with Newsgroups 15 and 19, because 17 and 10 are much more distinct.\n",
    "\n",
    "We continue these experiments to observe continued success:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running xval\n",
      "news/: pos_17 vs neg_17, using word presence/absence features\n",
      "features weighted by evidence they give of class\n",
      "classified by logistic regression\n",
      "average accuracy 0.9545212765957449 (std 0.013506344601696401)\n",
      "[0.94680851063829785, 0.9521276595744681, 0.9521276595744681, 0.96276595744680848, 0.94148936170212771, 0.97872340425531912, 0.96808510638297873, 0.96276595744680848, 0.93617021276595747, 0.94148936170212771, 0.97872340425531912, 0.92553191489361697, 0.95744680851063835, 0.9521276595744681, 0.96276595744680848, 0.9521276595744681, 0.9521276595744681, 0.94680851063829785, 0.94680851063829785, 0.97340425531914898],20\n"
     ]
    }
   ],
   "source": [
    "if selected(\"expt_11_20_c17\") :\n",
    "    expt_11_20_c17 = Experiment.transform(expt_11_c17,\n",
    "                             wang_manning_weights(expt_11_c17),\n",
    "                             \"features weighted by evidence they give of class\",\n",
    "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                             \"logistic regression\")\n",
    "    expt_11_20_c17.xval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running xval\n",
      "news/: pos_17 vs neg_17, using word count features\n",
      "features weighted by inverse document frequency\n",
      "classified by logistic regression\n",
      "average accuracy 0.9140957446808509 (std 0.011807432383203784)\n",
      "[0.90957446808510634, 0.92021276595744683, 0.8936170212765957, 0.89893617021276595, 0.93085106382978722, 0.9042553191489362, 0.90957446808510634, 0.9042553191489362, 0.92553191489361697, 0.89893617021276595, 0.91489361702127658, 0.93085106382978722, 0.93085106382978722, 0.92021276595744683, 0.91489361702127658, 0.8936170212765957, 0.92553191489361697, 0.92021276595744683, 0.91489361702127658, 0.92021276595744683],20\n"
     ]
    }
   ],
   "source": [
    "if selected(\"expt_10_20_c17\") :\n",
    "    expt_10_20_c17 = Experiment.transform(expt_10_c17,\n",
    "                             idf_weights(expt_10_c17),\n",
    "                             \"features weighted by inverse document frequency\",\n",
    "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                             \"logistic regression\")\n",
    "    expt_10_20_c17.xval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the experiment using IDF weighting is slightly lower (91.5%), but accuracy using Wang-Manning weights increases to 95%. Perhaps inverse document frequency is not as informative as the words' relevance to their classification. This makes sense--with two classes so wildly different from each other, the words within each document should be very useful indicators of the document's newsgroup.\n",
    "\n",
    "Now, we introduce word embeddings and LSA transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running xval\n",
      "news/: pos_17 vs neg_17, using word count features\n",
      "enriched via word embeddings\n",
      "classified by logistic regression\n",
      "average accuracy 0.9223404255319148 (std 0.008308776250964534)\n",
      "[0.90957446808510634, 0.93085106382978722, 0.92553191489361697, 0.92021276595744683, 0.93085106382978722, 0.90957446808510634, 0.92553191489361697, 0.93617021276595747, 0.91489361702127658, 0.91489361702127658, 0.90957446808510634, 0.93085106382978722, 0.92021276595744683, 0.92021276595744683, 0.92021276595744683, 0.93617021276595747, 0.91489361702127658, 0.92021276595744683, 0.93085106382978722, 0.92553191489361697],20\n",
      "\n",
      "\n",
      "Running xval\n",
      "news/: pos_17 vs neg_17, using word count features\n",
      "transformed via LSA(100)\n",
      "classified by logistic regression\n",
      "average accuracy 0.9039893617021276 (std 0.0063995794763382656)\n",
      "[0.90957446808510634, 0.90957446808510634, 0.9042553191489362, 0.89893617021276595, 0.9042553191489362, 0.90957446808510634, 0.89893617021276595, 0.9042553191489362, 0.89893617021276595, 0.88829787234042556, 0.9042553191489362, 0.8936170212765957, 0.9042553191489362, 0.90957446808510634, 0.90957446808510634, 0.90957446808510634, 0.89893617021276595, 0.90957446808510634, 0.89893617021276595, 0.91489361702127658],20\n",
      "\n",
      "\n",
      "Running xval\n",
      "news/: pos_17 vs neg_17, using word presence/absence features\n",
      "enriched via word embeddings\n",
      "classified by logistic regression\n",
      "average accuracy 0.9436170212765962 (std 0.007960973163348806)\n",
      "[0.94148936170212771, 0.9521276595744681, 0.93617021276595747, 0.94680851063829785, 0.94148936170212771, 0.93085106382978722, 0.93617021276595747, 0.94680851063829785, 0.94148936170212771, 0.93617021276595747, 0.94680851063829785, 0.94680851063829785, 0.93617021276595747, 0.94680851063829785, 0.95744680851063835, 0.96276595744680848, 0.94148936170212771, 0.94680851063829785, 0.93085106382978722, 0.94680851063829785],20\n",
      "\n",
      "\n",
      "Running xval\n",
      "news/: pos_17 vs neg_17, using word presence/absence features\n",
      "features weighted by evidence they give of class\n",
      "enriched via word embeddings\n",
      "classified by logistic regression\n",
      "average accuracy 0.9603723404255321 (std 0.007611217032582007)\n",
      "[0.95744680851063835, 0.96276595744680848, 0.97340425531914898, 0.9521276595744681, 0.95744680851063835, 0.96276595744680848, 0.96276595744680848, 0.95744680851063835, 0.95744680851063835, 0.96276595744680848, 0.96808510638297873, 0.96276595744680848, 0.96276595744680848, 0.95744680851063835, 0.96276595744680848, 0.94680851063829785, 0.97340425531914898, 0.96808510638297873, 0.95744680851063835, 0.94148936170212771],20\n"
     ]
    }
   ],
   "source": [
    "if selected(\"expt_10_211_c17\") :\n",
    "    expt_10_211_c17 = Experiment.transform(expt_10_c17,\n",
    "                             add_embeddings(expt_10_c17, e),\n",
    "                             \"enriched via word embeddings\",\n",
    "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                            \"logistic regression\")\n",
    "    expt_10_211_c17.xval()\n",
    "\n",
    "print ('\\n')\n",
    "\n",
    "if selected (\"expt_10_212_c17\") :\n",
    "    expt_10_212_c17 = Experiment.transform(expt_10_c17,\n",
    "                             dimensionality_reduction(expt_10_c17, 100),\n",
    "                             \"transformed via LSA(100)\",\n",
    "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                           \"logistic regression\")\n",
    "    expt_10_212_c17.xval()\n",
    "\n",
    "print ('\\n')\n",
    "\n",
    "\n",
    "if selected(\"expt_11_211_c17\") :\n",
    "    expt_11_211_c17 = Experiment.transform(expt_11_c17,\n",
    "                             add_embeddings(expt_11_c17, e),\n",
    "                             \"enriched via word embeddings\",\n",
    "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                            \"logistic regression\")\n",
    "    expt_11_211_c17.xval()\n",
    "    \n",
    "print ('\\n')\n",
    "\n",
    "\n",
    "if selected(\"expt_11_20_211_c17\") :\n",
    "    expt_11_20_211_c17 = Experiment.transform(expt_11_20_c17,\n",
    "                             add_embeddings(expt_11_20_c17, e),\n",
    "                             \"enriched via word embeddings\",\n",
    "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                            \"logistic regression\")\n",
    "    expt_11_20_211_c17.xval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using word embeddings with word count features, the accuracy of the experiment remains around 92%; these do not appear to be helpful in classifying documents by their newsgroup with word count features. This is similar to the experiment in 1.1. Also similar to 1.1, the experiment's accuracy is greatly reduced by LSA. Interestingly, however, 1.1 and 1.2 have similar accuracies when LSA is used (around 65%), despite the pairs of newsgroups being different in nature. This could be simply due to the fact that LSA reduces detail enough for both experiments to be equivalent to something slightly better than a blind guess (50%). \n",
    "\n",
    "Unsurprisingly, the base accuracy of the experiment using word presence/absence features is improved by the use of word embeddings and word presence/absence features, and by the addition of Wang-Manning weights to this scheme (94% and 96%, respectively). \n",
    "\n",
    "We now introduce bigram data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running xval\n",
      "news/: pos_17 vs neg_17, using word and bigram presence/absence features\n",
      "classified by logistic regression\n",
      "average accuracy 0.9138297872340424 (std 0.013414383205232427)\n",
      "[0.93085106382978722, 0.88829787234042556, 0.89893617021276595, 0.91489361702127658, 0.92553191489361697, 0.88297872340425532, 0.92553191489361697, 0.89893617021276595, 0.90957446808510634, 0.93085106382978722, 0.92553191489361697, 0.90957446808510634, 0.90957446808510634, 0.93085106382978722, 0.91489361702127658, 0.92553191489361697, 0.90957446808510634, 0.90957446808510634, 0.91489361702127658, 0.92021276595744683],20\n",
      "\n",
      "\n",
      "Running xval\n",
      "news/: pos_17 vs neg_17, using word and bigram presence/absence features\n",
      "features weighted by evidence they give of class\n",
      "classified by logistic regression\n",
      "average accuracy 0.9372340425531913 (std 0.009151409858556009)\n",
      "[0.94148936170212771, 0.93617021276595747, 0.93617021276595747, 0.94148936170212771, 0.93617021276595747, 0.94680851063829785, 0.95744680851063835, 0.93085106382978722, 0.93617021276595747, 0.92553191489361697, 0.92553191489361697, 0.93085106382978722, 0.93085106382978722, 0.93085106382978722, 0.92553191489361697, 0.94148936170212771, 0.92553191489361697, 0.94680851063829785, 0.94680851063829785, 0.9521276595744681],20\n",
      "\n",
      "\n",
      "Running xval\n",
      "news/: pos_17 vs neg_17, using word and bigram presence/absence features\n",
      "with word embedding features added\n",
      "classified by logistic regression\n",
      "average accuracy 0.9417553191489365 (std 0.011090776309570774)\n",
      "[0.92021276595744683, 0.95744680851063835, 0.94148936170212771, 0.93085106382978722, 0.93085106382978722, 0.94680851063829785, 0.9521276595744681, 0.9521276595744681, 0.93085106382978722, 0.96276595744680848, 0.93617021276595747, 0.94680851063829785, 0.9521276595744681, 0.94680851063829785, 0.94148936170212771, 0.94148936170212771, 0.93085106382978722, 0.9521276595744681, 0.92553191489361697, 0.93617021276595747],20\n",
      "\n",
      "\n",
      "Running xval\n",
      "news/: pos_17 vs neg_17, using word and bigram presence/absence features\n",
      "features weighted by evidence they give of class\n",
      "with word embedding features added\n",
      "classified by logistic regression\n",
      "average accuracy 0.9492021276595748 (std 0.007423024328166424)\n",
      "[0.94680851063829785, 0.94680851063829785, 0.94680851063829785, 0.95744680851063835, 0.94680851063829785, 0.95744680851063835, 0.95744680851063835, 0.94148936170212771, 0.94680851063829785, 0.9521276595744681, 0.96276595744680848, 0.94680851063829785, 0.94148936170212771, 0.94680851063829785, 0.9521276595744681, 0.95744680851063835, 0.94680851063829785, 0.94148936170212771, 0.95744680851063835, 0.93085106382978722],20\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bigram_data = newsreader.DataManager(train_dir + class17_pos,\n",
    "                                       train_dir + class17_neg,\n",
    "                                       test_dir + class17_pos,\n",
    "                                       test_dir + class17_neg,\n",
    "                                       use_bigram_features,\n",
    "                                       make_boolean_features(count_bigram_features),\n",
    "                                       dev_dir + class17_pos if dev_dir else None,\n",
    "                                       dev_dir + class17_neg if dev_dir else None,\n",
    "                                       strip_metadata=has_bad_metadata)\n",
    "\n",
    "bigram_data.initialize(build_cache=True)\n",
    "\n",
    "if selected(\"expt_22_c17\") :\n",
    "    expt_22_c17 = Experiment(bigram_data,\n",
    "                   \"{}: {} vs {}, using word and bigram presence/absence features\".format(all_data, class17_pos, class17_neg),\n",
    "                   sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                   \"logistic regression\")\n",
    "    expt_22_c17.initialize()\n",
    "    expt_22_c17.xval()\n",
    "\n",
    "print ('\\n')\n",
    "\n",
    "if selected(\"expt_22_220_c17\") :\n",
    "    expt_22_220_c17 = Experiment.transform(expt_22_c17,\n",
    "                             wang_manning_weights(expt_22_c17),\n",
    "                             \"features weighted by evidence they give of class\",\n",
    "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                             \"logistic regression\")\n",
    "    expt_22_220_c17.initialize()\n",
    "    expt_22_220_c17.xval()\n",
    "\n",
    "print ('\\n')\n",
    "\n",
    "\n",
    "if selected(\"expt_22_221_\") :\n",
    "    expt_22_221_c17 = Experiment.transform(expt_22_c17,\n",
    "                             add_embeddings(expt_22_c17, e),\n",
    "                             \"with word embedding features added\",\n",
    "                            sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                            \"logistic regression\")\n",
    "    expt_22_221_c17.xval()\n",
    "\n",
    "print ('\\n')\n",
    "\n",
    "\n",
    "if selected(\"expt_22_220_221_c17\") :\n",
    "    expt_22_220_221_c17 = Experiment.transform(expt_22_220_c17,\n",
    "                             add_embeddings(expt_22_220_c17, e),\n",
    "                             \"with word embedding features added\",\n",
    "                            sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                            \"logistic regression\")\n",
    "    expt_22_220_221_c17.xval()\n",
    "\n",
    "print ('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of word and bigram presence/absence features lowered the base accuracy slightly (by about 1%), whereas this scheme with added Wang-Manning weights increased the accuracy to about 94%. However, this accuracy is lower than that of the experiment with Wang-Manning weights that did not include bigram presence/absence features. It is possible that the bigrams within these documents are not indicative of the newsgroups.\n",
    "\n",
    "Similar to before, the addition of word embedding does not seem to improve accuracy significantly; the accuracy stays within 1 percentage point of the similar experiments that did not include bigram presence/absence features.\n",
    "\n",
    "Finally, we run the experiments using only 10% of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running xval\n",
      "news/: pos_17 vs neg_17, using word presence/absence features\n",
      "considering 10% training data\n",
      "classified by logistic regression\n",
      "average accuracy 0.8678191489361703 (std 0.020619850825746824)\n",
      "[0.8936170212765957, 0.83510638297872342, 0.8457446808510638, 0.84042553191489366, 0.86170212765957444, 0.87765957446808507, 0.88829787234042556, 0.86170212765957444, 0.8563829787234043, 0.86170212765957444, 0.8457446808510638, 0.86702127659574468, 0.87234042553191493, 0.88829787234042556, 0.90957446808510634, 0.8457446808510638, 0.8936170212765957, 0.85106382978723405, 0.86702127659574468, 0.8936170212765957],20\n",
      "\n",
      "\n",
      "Running xval\n",
      "news/: pos_17 vs neg_17, using word presence/absence features\n",
      "enriched via word embeddings\n",
      "considering 10% training data\n",
      "classified by logistic regression\n",
      "average accuracy 0.9194148936170212 (std 0.01598180764635491)\n",
      "[0.93617021276595747, 0.94148936170212771, 0.93617021276595747, 0.92553191489361697, 0.8936170212765957, 0.90957446808510634, 0.9042553191489362, 0.92021276595744683, 0.9042553191489362, 0.92021276595744683, 0.92553191489361697, 0.92021276595744683, 0.94680851063829785, 0.88297872340425532, 0.91489361702127658, 0.93085106382978722, 0.9042553191489362, 0.91489361702127658, 0.93617021276595747, 0.92021276595744683],20\n",
      "\n",
      "\n",
      "Running xval\n",
      "news/: pos_17 vs neg_17, using word presence/absence features\n",
      "features weighted by evidence they give of class\n",
      "considering 10% training data\n",
      "classified by logistic regression\n",
      "average accuracy 0.913563829787234 (std 0.017551176544570665)\n",
      "[0.9042553191489362, 0.90957446808510634, 0.90957446808510634, 0.94148936170212771, 0.93085106382978722, 0.93617021276595747, 0.9042553191489362, 0.93617021276595747, 0.9042553191489362, 0.89893617021276595, 0.93085106382978722, 0.89893617021276595, 0.94148936170212771, 0.92021276595744683, 0.92553191489361697, 0.88829787234042556, 0.9042553191489362, 0.90957446808510634, 0.88297872340425532, 0.8936170212765957],20\n",
      "\n",
      "\n",
      "Running xval\n",
      "news/: pos_17 vs neg_17, using word presence/absence features\n",
      "features weighted by evidence they give of class\n",
      "enriched via word embeddings\n",
      "considering 10% training data\n",
      "classified by logistic regression\n",
      "average accuracy 0.9465425531914896 (std 0.013188379008741962)\n",
      "[0.93085106382978722, 0.93617021276595747, 0.94680851063829785, 0.93617021276595747, 0.9521276595744681, 0.9521276595744681, 0.96276595744680848, 0.9521276595744681, 0.9521276595744681, 0.95744680851063835, 0.96808510638297873, 0.9521276595744681, 0.9521276595744681, 0.93085106382978722, 0.96276595744680848, 0.91489361702127658, 0.92553191489361697, 0.94680851063829785, 0.9521276595744681, 0.94680851063829785],20\n"
     ]
    }
   ],
   "source": [
    "if selected(\"expt_11_231_c17\") :\n",
    "    expt_11_231_c17 = Experiment.transform(expt_11_c17,\n",
    "                             limit_training(0.1),\n",
    "                             \"considering 10% training data\",\n",
    "                            sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                            \"logistic regression\")\n",
    "    expt_11_231_c17.xval()\n",
    "\n",
    "print ('\\n')\n",
    "\n",
    "if selected(\"expt_11_211_232_c17\") :\n",
    "    expt_11_231_232_c17 = Experiment.transform(expt_11_211_c17,\n",
    "                             limit_training(0.1),\n",
    "                             \"considering 10% training data\",\n",
    "                            sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                            \"logistic regression\")\n",
    "    expt_11_231_232_c17.xval()\n",
    "\n",
    "print ('\\n')\n",
    "\n",
    "\n",
    "if selected(\"expt_11_20_233_c17\") :\n",
    "    expt_11_20_232_c17 = Experiment.transform(expt_11_20_c17,\n",
    "                             limit_training(0.1),\n",
    "                             \"considering 10% training data\",\n",
    "                            sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                            \"logistic regression\")\n",
    "    expt_11_20_232_c17.xval()\n",
    "    \n",
    "print ('\\n')\n",
    "\n",
    "\n",
    "if selected(\"expt_11_20_211_234_c17\") :\n",
    "    expt_11_20_211_232_c17 = Experiment.transform(expt_11_20_211_c17,\n",
    "                             limit_training(0.1),\n",
    "                             \"considering 10% training data\",\n",
    "                            sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                            \"logistic regression\")\n",
    "    expt_11_20_211_232_c17.xval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that using only 10% of the training data in 1.2 is not as detrimental to accuracy as it was in 1.1. This is likely due to the fact that the two newsgroups being compared are distinct enough to provide adequate training for the model with much less data than before. It should be noted that the accuracy does fall slightly when using solely word presence/absence features, word embeddings, or Wang-Manning weights, but it is improved to about 95% with the usage of all three of these methods together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above exploration, it becomes evident that the pairs of newsgroups that appear more similar are more difficult to classify (as a binary, or positive/negative classification) than those which appear different. This leads to the following hypothesis:\n",
    "\n",
    "**When compared to ALL other newsgroups, a newsgroup that is similar to many others in the dataset will yield a much lower classification accuracy than a newsgroup that is distinct from many others in the dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimental Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the cosine similarity between all newsgroups will be calculated and represented in heatmap form, using TFIDF representations of words within documents and newsgroups. Then, this heatmap will be used to select two newsgroups. One of the newsgroups selected (\"Newsgroup A\") will be similar to the other newsgroups (the cells corresponding to its row in the heatmap matrix will be noticeably lighter) and the other (\"Newsgroup B\") will be different from the other newsgroups (its rows will consist of visibly darker cells).\n",
    "\n",
    "Then, the classification accuracy will be measured for each of these newsgroups using various experimental methods. A \"positive\" classification will indicate that a certain document is part of the newsgroup in question, whereas a \"negative\" classification will indicate that the document is part of any of the other 19 newsgroups. \n",
    "\n",
    "From Part 1 of the experiment, it appears that bigram representations are not a useful way to classify documents. Word embeddings also appear to be ineffective unless combined with Wang-Manning weights and/or word presence/absence features. These experiments, expt_11\\_211\\_ and expt_11_20\\_211\\_ , yielded fairly high or baseline-equivalent accuracy in both 1.1 and 1.2. It will be interesting to see whether this is the case in broader comparisons such as the ones in this experiment. Thus, these experiments will be used, along with the basic experiments preceding them. Finally, IDF weights with word count feature representations will also be used to compare the two classifications. The training set will not be limited; this appears to be detrimental to the accuracy in most cases.\n",
    "\n",
    "**The hypothesis will be considered correct if Newsgroup A yields a significantly lower classification accuracy than Newsgroup B throughout the experiments conducted.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Calculating Average Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, marginal unit vector sums are required to calculate the average cosine similarity between two newsgroups, which will be important to our hypothesis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def marginal_unit_vector_sums(partitions, tfidf_matrix):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - partitions (list of lists): each sublist at an index represents the document ids with that index's newsgroup ID\n",
    "        - tfidf_matrix (matrix): matrix of newsgroups/documents represented in TFIDF form\n",
    "    Returns:\n",
    "        - sums (defaultdict): dict of marginal unit vector sums, key = newsgroup ID, value = sums\n",
    "    \"\"\"\n",
    "    n_partitions = len(partitions)\n",
    "    sums = defaultdict()\n",
    "    for p in range(n_partitions):\n",
    "        sums[p] = 0\n",
    "\n",
    "    for n_id in range(n_partitions):\n",
    "        for doc_index in partitions[n_id]:\n",
    "            doc_vector = tfidf_matrix.getrow(doc_index).toarray()[0]\n",
    "            sums[n_id] = add(sums[n_id], doc_vector/norm(doc_vector))\n",
    "\n",
    "    return sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, score_map (a 2d array) is populated with average cosine similarities based on the newsgroups' TFIDF representations. For these calculations, the 20_newsgroups dataset was imported from sklearn.datasets for convenience's sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmer colors in heatmap indicate higher cosine similarity: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEICAYAAABF82P+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZo1cIMzMrNH/BwjWfx0XRb9yAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a0cdc8cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_newsgroups = 20\n",
    "training_set = fetch_20newsgroups()\n",
    "token_model = CountVectorizer().fit_transform(training_set.data)\n",
    "tfidf_model = TfidfTransformer().fit_transform(token_model)\n",
    "score_map = [[-1 for i in range(n_newsgroups)] for j in range(n_newsgroups)]\n",
    "\n",
    "#sort newsgroups using training set \n",
    "doc_partitions = [[] for x in range(n_newsgroups)]\n",
    "for i in range(len(training_set.target)):\n",
    "    tag = training_set.target[i]\n",
    "    doc_partitions[tag].append(i)\n",
    "\n",
    "#marginal unit vector sums for TFIDF \n",
    "doc_sums = marginal_unit_vector_sums(doc_partitions, tfidf_model)\n",
    "\n",
    "for x in range(n_newsgroups):\n",
    "    for y in range(n_newsgroups):\n",
    "        iterations = len(doc_partitions[x]*len(doc_partitions[y]))\n",
    "        #reduce number of calculations required by eliminating duplicates\n",
    "        if score_map[y][x] != -1:\n",
    "            score_map[x][y] = score_map[y][x]\n",
    "        else:\n",
    "            score_map[x][y] = dot(doc_sums[x], doc_sums[y])/iterations\n",
    "\n",
    "# show matrix as heatmap\n",
    "\n",
    "print ('Warmer colors in heatmap indicate higher cosine similarity: \\n\\n')\n",
    "plt.title('Cosine Similarities Among Newsgroups (TFIDF Representation)')\n",
    "plt.xticks(np.arange(0, 20, 1.0))\n",
    "plt.yticks(np.arange(0, 20, 1.0))\n",
    "plt.imshow(score_map, cmap='jet', interpolation='nearest')\n",
    "plt.show()\n",
    "plt.savefig('heatmap.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, some quick analysis of the newsgroup category names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newsgroup 0 Category: alt.atheism\n",
      "Newsgroup 1 Category: comp.graphics\n",
      "Newsgroup 2 Category: comp.os.ms-windows.misc\n",
      "Newsgroup 3 Category: comp.sys.ibm.pc.hardware\n",
      "Newsgroup 4 Category: comp.sys.mac.hardware\n",
      "Newsgroup 5 Category: comp.windows.x\n",
      "Newsgroup 6 Category: misc.forsale\n",
      "Newsgroup 7 Category: rec.autos\n",
      "Newsgroup 8 Category: rec.motorcycles\n",
      "Newsgroup 9 Category: rec.sport.baseball\n",
      "Newsgroup 10 Category: rec.sport.hockey\n",
      "Newsgroup 11 Category: sci.crypt\n",
      "Newsgroup 12 Category: sci.electronics\n",
      "Newsgroup 13 Category: sci.med\n",
      "Newsgroup 14 Category: sci.space\n",
      "Newsgroup 15 Category: soc.religion.christian\n",
      "Newsgroup 16 Category: talk.politics.guns\n",
      "Newsgroup 17 Category: talk.politics.mideast\n",
      "Newsgroup 18 Category: talk.politics.misc\n",
      "Newsgroup 19 Category: talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print ('Newsgroup {} Category: {}'.format(i, training_set.target_names[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the heatmap, the Atheism category appears to be very similar to the others (especially the last five categories regarding religion and politics), and the Miscellaneous \"For Sale\" category appears to be very different from the others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Setup\n",
    "\n",
    "Now, we begin setting up the experiments. class0_pos refers to the directory which contains examples of the Atheism class, while class0_neg refers to the directory containing *all other documents.* The same structure applies to class6_pos and class6_neg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class0_pos = \"pos_0\"\n",
    "class0_neg = \"neg_0\"\n",
    "class6_pos = \"pos_6\"\n",
    "class6_neg = \"neg_6\"\n",
    "\n",
    "count0_data = newsreader.DataManager(train_dir + class0_pos,\n",
    "                                       train_dir + class0_neg,\n",
    "                                       test_dir + class0_pos,\n",
    "                                       test_dir + class0_neg,\n",
    "                                       use_default_features(v),\n",
    "                                       count_features,\n",
    "                                       dev_dir + class0_pos if dev_dir else None,\n",
    "                                       dev_dir if dev_dir else None,\n",
    "                                       strip_metadata=has_bad_metadata)\n",
    "\n",
    "count6_data = newsreader.DataManager(train_dir + class6_pos,\n",
    "                                       train_dir + class6_neg,\n",
    "                                       test_dir + class6_pos,\n",
    "                                       test_dir + class6_neg,\n",
    "                                       use_default_features(v),\n",
    "                                       count_features,\n",
    "                                       dev_dir + class6_pos if dev_dir else None,\n",
    "                                       dev_dir if dev_dir else None,\n",
    "                                       strip_metadata=has_bad_metadata)\n",
    "\n",
    "\n",
    "\n",
    "count0_data.initialize(build_cache=True)\n",
    "count6_data.initialize(build_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the boolean data representations (presence/absence features) of Newsgroup 0 and Newsgroup 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boolean_data_0 = newsreader.DataManager(train_dir + class0_pos,\n",
    "                                         train_dir + class0_neg,\n",
    "                                         test_dir + class0_pos,\n",
    "                                         test_dir + class0_neg,\n",
    "                                         use_default_features(v),\n",
    "                                         make_boolean_features(count_features),\n",
    "                                         dev_dir + class0_pos if dev_dir else None,\n",
    "                                         dev_dir + class0_neg if dev_dir else None,\n",
    "                                         strip_metadata=has_bad_metadata)\n",
    "\n",
    "boolean_data_6 = newsreader.DataManager(train_dir + class6_pos,\n",
    "                                         train_dir + class6_neg,\n",
    "                                         test_dir + class6_pos,\n",
    "                                         test_dir + class6_neg,\n",
    "                                         use_default_features(v),\n",
    "                                         make_boolean_features(count_features),\n",
    "                                         dev_dir + class6_pos if dev_dir else None,\n",
    "                                         dev_dir + class6_neg if dev_dir else None,\n",
    "                                         strip_metadata=has_bad_metadata)\n",
    "\n",
    "\n",
    "boolean_data_0.initialize(build_cache=True)\n",
    "boolean_data_6.initialize(build_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Initial experiments\n",
    "\n",
    "#### 4.2.1: Experiment 10\n",
    "\n",
    "Here, expt_10 is run for Newsgroup 0 classification and Newsgroup 6 classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running xval\n",
      "news/: pos_0 vs neg_0, using word count features\n",
      "classified by logistic regression\n",
      "average accuracy 0.6287974683544305 (std 0.020632814319877263)\n",
      "[0.62658227848101267, 0.65822784810126578, 0.67088607594936711, 0.61392405063291144, 0.63291139240506333, 0.63291139240506333, 0.60126582278481011, 0.62658227848101267, 0.65189873417721522, 0.60126582278481011, 0.63924050632911389, 0.63291139240506333, 0.60126582278481011, 0.65189873417721522, 0.61392405063291144, 0.61392405063291144, 0.60126582278481011, 0.62658227848101267, 0.65822784810126578, 0.620253164556962],20\n"
     ]
    }
   ],
   "source": [
    "if selected(\"expt_10_c0\"):\n",
    "    expt_10_c0 = Experiment(count0_data,\n",
    "                       \"{}: {} vs {}, using word count features\".format(all_data, class0_pos, class0_neg),\n",
    "                       sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                       \"logistic regression\")\n",
    "    expt_10_c0.initialize()\n",
    "    expt_10_c0.xval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for Class 0, as expected, is not very high; its documents are similar to various other documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running xval\n",
      "news/: pos_6 vs neg_6, using word count features\n",
      "classified by logistic regression\n",
      "average accuracy 0.8528350515463918 (std 0.010992200057802127)\n",
      "[0.87628865979381443, 0.83505154639175261, 0.86082474226804129, 0.865979381443299, 0.82989690721649489, 0.86082474226804129, 0.85051546391752575, 0.84536082474226804, 0.84536082474226804, 0.85567010309278346, 0.85567010309278346, 0.84536082474226804, 0.85567010309278346, 0.85051546391752575, 0.85051546391752575, 0.86082474226804129, 0.865979381443299, 0.84020618556701032, 0.86082474226804129, 0.84536082474226804],20\n"
     ]
    }
   ],
   "source": [
    "if selected(\"expt_10_c6\"):\n",
    "    expt_10_c6 = Experiment(count6_data,\n",
    "                       \"{}: {} vs {}, using word count features\".format(all_data, class6_pos, class6_neg),\n",
    "                       sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                       \"logistic regression\")\n",
    "    expt_10_c6.initialize()\n",
    "    expt_10_c6.xval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification for Class 6 is much more accurate, meaning that the model is better able to predict whether a document is in this class or not.\n",
    "\n",
    "#### 4.2.2: Experiment 11\n",
    "\n",
    "Now, we use boolean data for experiment 11:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running xval\n",
      "news/: pos_0 vs neg_0, using word presence/absence features\n",
      "classified by logistic regression\n",
      "average accuracy 0.5984177215189874 (std 0.005092872449187067)\n",
      "[0.59493670886075944, 0.59493670886075944, 0.59493670886075944, 0.60126582278481011, 0.60759493670886078, 0.59493670886075944, 0.60759493670886078, 0.59493670886075944, 0.59493670886075944, 0.60126582278481011, 0.59493670886075944, 0.59493670886075944, 0.59493670886075944, 0.60759493670886078, 0.60126582278481011, 0.60126582278481011, 0.59493670886075944, 0.60126582278481011, 0.60126582278481011, 0.58860759493670889],20\n"
     ]
    }
   ],
   "source": [
    "if selected(\"expt_11_c0\") :\n",
    "    expt_11_c0 = Experiment(boolean_data_0,\n",
    "                         \"{}: {} vs {}, using word presence/absence features\".format(all_data, class0_pos, class0_neg),\n",
    "                         sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                         \"logistic regression\")\n",
    "    expt_11_c0.initialize()\n",
    "    expt_11_c0.xval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low classification accuracy--only 10% better than a blind guess--is evident for Class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running xval\n",
      "news/: pos_6 vs neg_6, using word presence/absence features\n",
      "classified by logistic regression\n",
      "average accuracy 0.8409793814432991 (std 0.005473391902472997)\n",
      "[0.84020618556701032, 0.83505154639175261, 0.84536082474226804, 0.83505154639175261, 0.84536082474226804, 0.84536082474226804, 0.84020618556701032, 0.83505154639175261, 0.84020618556701032, 0.84020618556701032, 0.84536082474226804, 0.83505154639175261, 0.85567010309278346, 0.84536082474226804, 0.83505154639175261, 0.84536082474226804, 0.84020618556701032, 0.83505154639175261, 0.83505154639175261, 0.84536082474226804],20\n"
     ]
    }
   ],
   "source": [
    "if selected(\"expt_11_c6\") :\n",
    "    expt_11_c6 = Experiment(boolean_data_6,\n",
    "                         \"{}: {} vs {}, using word presence/absence features\".format(all_data, class6_pos, class6_neg),\n",
    "                         sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                         \"logistic regression\")\n",
    "    expt_11_c6.initialize()\n",
    "    expt_11_c6.xval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for Class 6 classification using word presence/absence features is higher than that for Class 0, but lower than that of Class 6 using word count features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3: Adding Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1: IDF Weights\n",
    "\n",
    "IDF weights are used with the word count feature representations of Class 0 and Class 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " CLASS 0:\n",
      "Running xval\n",
      "news/: pos_0 vs neg_0, using word count features\n",
      "features weighted by inverse document frequency\n",
      "classified by logistic regression\n",
      "average accuracy 0.6325949367088608 (std 0.019037381319867135)\n",
      "[0.63291139240506333, 0.67088607594936711, 0.62658227848101267, 0.63924050632911389, 0.63291139240506333, 0.59493670886075944, 0.60126582278481011, 0.65822784810126578, 0.63924050632911389, 0.62658227848101267, 0.60759493670886078, 0.62658227848101267, 0.63924050632911389, 0.63924050632911389, 0.63924050632911389, 0.620253164556962, 0.67088607594936711, 0.620253164556962, 0.63291139240506333, 0.63291139240506333],20\n",
      "\n",
      " CLASS 6:\n",
      "Running xval\n",
      "news/: pos_6 vs neg_6, using word count features\n",
      "features weighted by inverse document frequency\n",
      "classified by logistic regression\n",
      "average accuracy 0.868814432989691 (std 0.021157580531433524)\n",
      "[0.88659793814432986, 0.88144329896907214, 0.88659793814432986, 0.89690721649484539, 0.86082474226804129, 0.83505154639175261, 0.87628865979381443, 0.88659793814432986, 0.87628865979381443, 0.86082474226804129, 0.82989690721649489, 0.88659793814432986, 0.83505154639175261, 0.83505154639175261, 0.86082474226804129, 0.88144329896907214, 0.84536082474226804, 0.88144329896907214, 0.88659793814432986, 0.88659793814432986],20\n"
     ]
    }
   ],
   "source": [
    "print ('\\n CLASS 0:')\n",
    "\n",
    "if selected(\"expt_10_20_c0\") :\n",
    "    expt_10_20_c0 = Experiment.transform(expt_10_c0,\n",
    "                             idf_weights(expt_10_c0),\n",
    "                             \"features weighted by inverse document frequency\",\n",
    "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                             \"logistic regression\")\n",
    "    expt_10_20_c0.xval()\n",
    "\n",
    "print ('\\n CLASS 6:')\n",
    "\n",
    "if selected(\"expt_10_20_c6\") :\n",
    "    expt_10_20_c6 = Experiment.transform(expt_10_c6,\n",
    "                             idf_weights(expt_10_c6),\n",
    "                             \"features weighted by inverse document frequency\",\n",
    "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                             \"logistic regression\")\n",
    "    expt_10_20_c6.xval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2: Wang-Manning Weights\n",
    "\n",
    "Wang-Manning weights are added for the word presence/absence feature representations of Class 0 and Class 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS 0:\n",
      "Running xval\n",
      "news/: pos_0 vs neg_0, using word presence/absence features\n",
      "features weighted by evidence they give of class\n",
      "classified by logistic regression\n",
      "average accuracy 0.5996835443037974 (std 0.009572457887502656)\n",
      "[0.59493670886075944, 0.60126582278481011, 0.58860759493670889, 0.58860759493670889, 0.61392405063291144, 0.60759493670886078, 0.61392405063291144, 0.60126582278481011, 0.61392405063291144, 0.61392405063291144, 0.59493670886075944, 0.59493670886075944, 0.60126582278481011, 0.60759493670886078, 0.60126582278481011, 0.58860759493670889, 0.59493670886075944, 0.58227848101265822, 0.60126582278481011, 0.58860759493670889],20\n",
      "\n",
      " CLASS 6:\n",
      "Running xval\n",
      "news/: pos_6 vs neg_6, using word presence/absence features\n",
      "features weighted by evidence they give of class\n",
      "classified by logistic regression\n",
      "average accuracy 0.8293814432989693 (std 0.012720580081704198)\n",
      "[0.82474226804123707, 0.82989690721649489, 0.80412371134020622, 0.82474226804123707, 0.87113402061855671, 0.80927835051546393, 0.82989690721649489, 0.82474226804123707, 0.82989690721649489, 0.82989690721649489, 0.84020618556701032, 0.83505154639175261, 0.82474226804123707, 0.82474226804123707, 0.82989690721649489, 0.82474226804123707, 0.83505154639175261, 0.82989690721649489, 0.82474226804123707, 0.84020618556701032],20\n"
     ]
    }
   ],
   "source": [
    "print('CLASS 0:')\n",
    "\n",
    "if selected(\"expt_11_20_c0\") :\n",
    "    expt_11_20_c0 = Experiment.transform(expt_11_c0,\n",
    "                             wang_manning_weights(expt_11_c0),\n",
    "                             \"features weighted by evidence they give of class\",\n",
    "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                             \"logistic regression\")\n",
    "    expt_11_20_c0.xval()\n",
    "\n",
    "print('\\n CLASS 6:')\n",
    "\n",
    "if selected(\"expt_11_20_c6\") :\n",
    "    expt_11_20_c6 = Experiment.transform(expt_11_c6,\n",
    "                             wang_manning_weights(expt_11_c6),\n",
    "                             \"features weighted by evidence they give of class\",\n",
    "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                             \"logistic regression\")\n",
    "    expt_11_20_c6.xval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4: Adding Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 Word Embeddings with Wang-Manning Weights\n",
    "\n",
    "Here, we integrate word embeddings into experiments 11_20_c0 and 11_20_c6: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS 0:\n",
      "Running xval\n",
      "news/: pos_0 vs neg_0, using word presence/absence features\n",
      "features weighted by evidence they give of class\n",
      "enriched via word embeddings\n",
      "classified by logistic regression\n",
      "average accuracy 0.5971518987341773 (std 0.010642513747341927)\n",
      "[0.58860759493670889, 0.60759493670886078, 0.60126582278481011, 0.59493670886075944, 0.58227848101265822, 0.58860759493670889, 0.58860759493670889, 0.59493670886075944, 0.60126582278481011, 0.59493670886075944, 0.58860759493670889, 0.57594936708860756, 0.58860759493670889, 0.60126582278481011, 0.60759493670886078, 0.620253164556962, 0.59493670886075944, 0.60126582278481011, 0.61392405063291144, 0.60759493670886078],20\n",
      "\n",
      "CLASS 6:\n",
      "Running xval\n",
      "news/: pos_6 vs neg_6, using word presence/absence features\n",
      "features weighted by evidence they give of class\n",
      "enriched via word embeddings\n",
      "classified by logistic regression\n",
      "average accuracy 0.8229381443298971 (std 0.009681257717006678)\n",
      "[0.83505154639175261, 0.81443298969072164, 0.82989690721649489, 0.81443298969072164, 0.82989690721649489, 0.7989690721649485, 0.81958762886597936, 0.82474226804123707, 0.81958762886597936, 0.82474226804123707, 0.81958762886597936, 0.81958762886597936, 0.81958762886597936, 0.82474226804123707, 0.84536082474226804, 0.82989690721649489, 0.81443298969072164, 0.82474226804123707, 0.83505154639175261, 0.81443298969072164],20\n"
     ]
    }
   ],
   "source": [
    "print ('CLASS 0:')\n",
    "\n",
    "if selected(\"expt_11_20_211_c0\") :\n",
    "    expt_11_20_211_c0 = Experiment.transform(expt_11_20_c0,\n",
    "                             add_embeddings(expt_11_20_c0, e),\n",
    "                             \"enriched via word embeddings\",\n",
    "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                            \"logistic regression\")\n",
    "    expt_11_20_211_c0.xval()\n",
    "\n",
    "print ('\\nCLASS 6:')\n",
    "\n",
    "if selected(\"expt_11_20_211_c6\") :\n",
    "    expt_11_20_211_c6 = Experiment.transform(expt_11_20_c6,\n",
    "                             add_embeddings(expt_11_20_c6, e),\n",
    "                             \"enriched via word embeddings\",\n",
    "                             sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                       penalty=\"elasticnet\",\n",
    "                                       n_iter=50),\n",
    "                            \"logistic regression\")\n",
    "    expt_11_20_211_c6.xval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 : Analysis of Results and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the individual patterns between the initial experiments in Part 1 are not consistent across the experiments in these new classification examples. For example, classification using word presence/absence features, Wang-Manning weights, and word embeddings combined was the most accurate in Part 1; however, here, the accuracy of this system was lower than that of the baseline experiments (10 and 11). This could be coincidental; it is likely that multiple iterations of these experiments need to be conducted, and the mean accuracies would present a better perspective on the effectiveness of each classification scheme. Significance tests would also need to be performed for a more in-depth analysis of these patterns.\n",
    "\n",
    "However, regardless of this, the hypothesis of this experiment does hold true. Classification of Class 0, Atheism, was fairly low-accuracy, while the classification of Class 6, Misc: For Sale, was consistently better across all experiments. This is likely due to the fact that the first newsgroup has a high cosine similarity to the rest of the dataset, while the second has a lower cosine similarity and is therefore easier to distinguish.\n",
    "\n",
    "A more in-depth analysis of the difference between the accuracies shows some interesting patterns (**NOTE: All calculations are based on the first iteration of these experiments; values may vary slightly)**:\n",
    "\n",
    "- It appears that word embeddings do not make much of a difference; the accuracies of classification with Wang-Manning weights and the accuracies of classification with word embeddings and Wang-Manning weights combined are very similar for class 0 and identical for class 6.\n",
    "\n",
    "- The average difference in accuracies between classification for class 0 and classification for class 6 across all 5 experiments was 23.09%. \n",
    "\n",
    "- The average accuracy for Class 0 classification was 61.01%.\n",
    "\n",
    "- The average accuracy for Class 6 classification was 84.11%.\n",
    "\n",
    "- The greatest difference in accuracy was for the second experiment, which used boolean features: 23.96%. \n",
    "\n",
    "- The least difference in accuracy was for the fourth experiment, which used Wang-Manning weights: 22.85%. However, the differences between other experiments were very similar to this number: \n",
    "\n",
    "    Exp 1 (word count features): 22.88%\n",
    "    \n",
    "    Exp 2 (boolean features): 23.96%\n",
    "    \n",
    "    Exp 3 (IDF weights): 22.81%\n",
    "    \n",
    "    Exp 4 (W-M weights): 22.85%\n",
    "    \n",
    "    Exp 5 (W-M weights + word embeddings w/ bool features): 22.98%\n",
    "\n",
    "Now, we perform significance tests to evaluate the hypothesis: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running xval\n",
      "news/: pos_0 vs neg_0, using word count features\n",
      "classified by logistic regression\n",
      "average accuracy 0.6231012658227848 (std 0.021876680927473885)\n",
      "[0.60759493670886078, 0.61392405063291144, 0.620253164556962, 0.60759493670886078, 0.63291139240506333, 0.63291139240506333, 0.63924050632911389, 0.60759493670886078, 0.60759493670886078, 0.66455696202531644, 0.620253164556962, 0.620253164556962, 0.61392405063291144, 0.60126582278481011, 0.69620253164556967, 0.60759493670886078, 0.620253164556962, 0.61392405063291144, 0.620253164556962, 0.61392405063291144],20\n",
      "Running xval\n",
      "news/: pos_0 vs neg_0, using word presence/absence features\n",
      "classified by logistic regression\n",
      "average accuracy 0.6 (std 0.004292613913370446)\n",
      "[0.60759493670886078, 0.59493670886075944, 0.59493670886075944, 0.60759493670886078, 0.59493670886075944, 0.60759493670886078, 0.60126582278481011, 0.60126582278481011, 0.59493670886075944, 0.60126582278481011, 0.60126582278481011, 0.60126582278481011, 0.60126582278481011, 0.59493670886075944, 0.59493670886075944, 0.60126582278481011, 0.60126582278481011, 0.59493670886075944, 0.60126582278481011, 0.60126582278481011],20\n",
      "Running xval\n",
      "news/: pos_0 vs neg_0, using word count features\n",
      "features weighted by inverse document frequency\n",
      "classified by logistic regression\n",
      "average accuracy 0.6310126582278481 (std 0.016516440950253015)\n",
      "[0.63924050632911389, 0.60759493670886078, 0.61392405063291144, 0.62658227848101267, 0.64556962025316456, 0.63924050632911389, 0.620253164556962, 0.62658227848101267, 0.62658227848101267, 0.63291139240506333, 0.66455696202531644, 0.62658227848101267, 0.62658227848101267, 0.61392405063291144, 0.65189873417721522, 0.60126582278481011, 0.62658227848101267, 0.65822784810126578, 0.65189873417721522, 0.620253164556962],20\n",
      "Running xval\n",
      "news/: pos_0 vs neg_0, using word presence/absence features\n",
      "features weighted by evidence they give of class\n",
      "classified by logistic regression\n",
      "average accuracy 0.6009493670886076 (std 0.012252190305446805)\n",
      "[0.60126582278481011, 0.61392405063291144, 0.60126582278481011, 0.57594936708860756, 0.59493670886075944, 0.58860759493670889, 0.620253164556962, 0.58860759493670889, 0.620253164556962, 0.60126582278481011, 0.60126582278481011, 0.61392405063291144, 0.58860759493670889, 0.59493670886075944, 0.59493670886075944, 0.61392405063291144, 0.60126582278481011, 0.58860759493670889, 0.620253164556962, 0.59493670886075944],20\n",
      "Running xval\n",
      "news/: pos_0 vs neg_0, using word presence/absence features\n",
      "features weighted by evidence they give of class\n",
      "enriched via word embeddings\n",
      "classified by logistic regression\n",
      "average accuracy 0.5886075949367088 (std 0.01059063324726678)\n",
      "[0.59493670886075944, 0.57594936708860756, 0.58227848101265822, 0.58860759493670889, 0.59493670886075944, 0.59493670886075944, 0.60126582278481011, 0.60759493670886078, 0.58227848101265822, 0.59493670886075944, 0.58860759493670889, 0.57594936708860756, 0.569620253164557, 0.58860759493670889, 0.57594936708860756, 0.59493670886075944, 0.57594936708860756, 0.60759493670886078, 0.59493670886075944, 0.58227848101265822],20\n",
      "Running xval\n",
      "news/: pos_6 vs neg_6, using word count features\n",
      "classified by logistic regression\n",
      "average accuracy 0.8505154639175257 (std 0.022170941409903687)\n",
      "[0.85567010309278346, 0.85567010309278346, 0.865979381443299, 0.85051546391752575, 0.76288659793814428, 0.85051546391752575, 0.865979381443299, 0.84536082474226804, 0.86082474226804129, 0.85567010309278346, 0.86082474226804129, 0.85051546391752575, 0.83505154639175261, 0.86082474226804129, 0.84536082474226804, 0.85051546391752575, 0.88144329896907214, 0.85051546391752575, 0.85567010309278346, 0.85051546391752575],20\n",
      "Running xval\n",
      "news/: pos_6 vs neg_6, using word presence/absence features\n",
      "classified by logistic regression\n",
      "average accuracy 0.8402061855670105 (std 0.005406231176134782)\n",
      "[0.84020618556701032, 0.84020618556701032, 0.84020618556701032, 0.84020618556701032, 0.84020618556701032, 0.84020618556701032, 0.83505154639175261, 0.83505154639175261, 0.83505154639175261, 0.84536082474226804, 0.83505154639175261, 0.84020618556701032, 0.85051546391752575, 0.83505154639175261, 0.84536082474226804, 0.85051546391752575, 0.83505154639175261, 0.83505154639175261, 0.83505154639175261, 0.85051546391752575],20\n",
      "Running xval\n",
      "news/: pos_6 vs neg_6, using word count features\n",
      "features weighted by inverse document frequency\n",
      "classified by logistic regression\n",
      "average accuracy 0.8644329896907218 (std 0.01748783036462618)\n",
      "[0.86082474226804129, 0.85567010309278346, 0.89690721649484539, 0.85567010309278346, 0.84536082474226804, 0.87628865979381443, 0.84536082474226804, 0.87113402061855671, 0.89175257731958768, 0.85567010309278346, 0.84020618556701032, 0.85051546391752575, 0.865979381443299, 0.89690721649484539, 0.84536082474226804, 0.87113402061855671, 0.85567010309278346, 0.85567010309278346, 0.86082474226804129, 0.89175257731958768],20\n",
      "Running xval\n",
      "news/: pos_6 vs neg_6, using word presence/absence features\n",
      "features weighted by evidence they give of class\n",
      "classified by logistic regression\n",
      "average accuracy 0.8329896907216497 (std 0.009698395733106588)\n",
      "[0.83505154639175261, 0.82989690721649489, 0.82989690721649489, 0.83505154639175261, 0.82989690721649489, 0.85051546391752575, 0.81443298969072164, 0.82474226804123707, 0.82474226804123707, 0.84536082474226804, 0.83505154639175261, 0.83505154639175261, 0.84536082474226804, 0.81958762886597936, 0.82989690721649489, 0.82989690721649489, 0.81958762886597936, 0.85051546391752575, 0.84020618556701032, 0.83505154639175261],20\n",
      "Running xval\n",
      "news/: pos_6 vs neg_6, using word presence/absence features\n",
      "features weighted by evidence they give of class\n",
      "enriched via word embeddings\n",
      "classified by logistic regression\n",
      "average accuracy 0.8234536082474229 (std 0.010163270182025964)\n",
      "[0.81443298969072164, 0.82989690721649489, 0.82474226804123707, 0.80412371134020622, 0.84020618556701032, 0.81443298969072164, 0.81443298969072164, 0.81958762886597936, 0.82989690721649489, 0.82989690721649489, 0.81958762886597936, 0.82474226804123707, 0.82989690721649489, 0.84536082474226804, 0.80927835051546393, 0.81958762886597936, 0.81958762886597936, 0.81443298969072164, 0.82989690721649489, 0.83505154639175261],20\n",
      "[[0.6231012658227848, 0.59999999999999998, 0.63101265822784813, 0.60094936708860758, 0.58860759493670878], [0.85051546391752575, 0.84020618556701054, 0.8644329896907218, 0.83298969072164974, 0.82345360824742286]]\n"
     ]
    }
   ],
   "source": [
    "#statistical significance tests\n",
    "\n",
    "expt_10_c0.initialize()\n",
    "expt_10_c6.initialize()\n",
    "expt_11_c0.initialize()\n",
    "expt_11_c6.initialize()\n",
    "\n",
    "EXPS = [[expt_10_c0.xval(), expt_11_c0.xval(), expt_10_20_c0.xval(), expt_11_20_c0.xval(), expt_11_20_211_c0.xval()],\n",
    "        [expt_10_c6.xval(), expt_11_c6.xval(), expt_10_20_c6.xval(), expt_11_20_c6.xval(), expt_11_20_211_c6.xval()]]\n",
    "\n",
    "#NUM_EXPS = len(EXPS)\n",
    "#expt_num = 0\n",
    "\n",
    "#for i in EXPS:\n",
    "#    for x in range(len(i)):\n",
    "#        accuracies[expt_num].append(i[x][0])\n",
    "#    expt_num += 1\n",
    "\n",
    "print(EXPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-22.030840947732109, pvalue=1.9028563059438678e-08)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.ttest_ind(EXPS[0], EXPS[1], equal_var=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-test p-value (1.9 * 10^-8) indicates that the likelihood of such a high difference between the Class 0 classification accuracy and that of Class 6 classification is very unlikely to be coincidental. Thus, the differences between these accuracies can be considered truly significant, and the hypothesis that Class 6 will be more accurately identified than Class 0 is true. In other words, documents from a newsgroup with higher cosine similarity to the overall dataset are more difficult to identify, even when using several different experimental conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
